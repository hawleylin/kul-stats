# 1.2 Statistical inference based on the likelihood function

Inference purely on likelihood function has not been developed to a full-blown statistical approach

Considered here as a **pre-cursor to Bayesian approach--贝叶斯方法的前驱**

## 1.2.1 The likelihood function

Likelihood function = **plausibility** of the observed data as a function of the parameters of the stochastic model--似然函数=观测数据作为随机模型参数的函数的似然性（似乎可信的程度）

Inference based on likelihood function is **QUITE different** from inference based on P-value

### Example I.7: A surgery experiment

New but rather complicated surgical technique. Surgeon operates n = 12 patients with s = 9 successes

Notation:

* Result on ith operation: success = 1, failure = 0
* Total experiment: n operations with s successes
* Sample $$\{y_1, y_2....., y_n\}=y$$ 
* Probability of success $$= p(y_i)=\theta$$ 
* ⇒ Binomial distribution:  Expresses probability of s successes out of n experiments.

### Binomial distribution

                                          $$f_\theta(s)=\begin{pmatrix}n\\\\s\end{pmatrix} \theta^s (1-\theta)^{n-s}\quad with\quad s=\displaystyle\sum_{i=1}^n y_i$$ 

$$\theta$$ ﬁxed & function of s: $$f_\theta(s)$$ is discrete distribution with $$\displaystyle \sum_{s=0}^nf_\theta(s)=1$$ 

 s ﬁxed & function of $$\theta$$:  binomial likelihood function  ****$$\bm{L(\theta|s)}$$ --**the plausibility of** $$\theta$$ **when data is given**

![](../.gitbook/assets/image%20%286%29.png)

## 1.2.2 The two likelihood principles

### Likelihood Principle 1

All evidence, which is obtained from an experiment, about an unknown quantity θ, is **contained in the likelihood function** of θ for the given data

* Maximal evidence for $$\hat\theta=0.75$$ 
* Likelihood ratio $$L(0.5|s)/L(0.75|s)$$ = relative evidence for 2 hypotheses $$\theta=0.5\ \&\ \theta=0.75$$ 
* Standardized likelihood: $$L_S(\theta|s)=L(\theta|s)/L(\hat\theta|s)$$ 
* $$L_S(0.5|s) =0.21=$$ test for hypothesis H 0 without involving ﬁctive data
* Interval of \(≥ 1/2 maximal\) evidence

### Likelihood Principle 2

Two likelihood functions for θ contain the same information about θ if they are **proportional** to each other

LP 2 = **Relative likelihood principle**  
⇒ When likelihood is proportional under two experimental conditions, then information about θ must be the same!

#### Example I.8: Another surgery experiment

Surgeon 1: \(Example I.7\) Operate 12 patients, observe 9 successes \(and 3 failures\)  
 $$s=\displaystyle\sum_{i=1}^n y_i$$ has a binomial distribution   
⇒ binomial likelihood $$L_1(\theta|s)=\begin{pmatrix}n\\s\end{pmatrix} \theta^s (1-\theta)^{n-s}$$ 

Surgeon 2: Operate n patients until 3 failures are observed   
$$s=\displaystyle\sum_{i=1}^n y_i$$ has a negative binomial distribution  
⇒ negative binomial likelihood $$L_2(\theta|s)=\begin{pmatrix}s+k-1\\s\end{pmatrix} \theta^s (1-\theta)^{k}$$ 

![&#x56E0;&#x4E3A;&#x4E24;&#x4E2A;likelihood function&#x662F;proportional&#x7684;](../.gitbook/assets/image%20%287%29.png)

### Conclusions

Frequentist conclusion ≠ Likelihood conclusion

Design aspects \(stopping rule\) are important in frequentist context



